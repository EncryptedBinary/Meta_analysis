{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Install packages "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-03T19:21:52.407629Z","iopub.status.busy":"2024-03-03T19:21:52.406788Z","iopub.status.idle":"2024-03-03T19:24:33.994801Z","shell.execute_reply":"2024-03-03T19:24:33.993858Z","shell.execute_reply.started":"2024-03-03T19:21:52.407595Z"},"trusted":true},"outputs":[],"source":["!pip install transformers trl accelerate torch bitsandbytes peft langchain datasets -qU"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:25:52.418629Z","iopub.status.busy":"2024-03-03T19:25:52.417677Z","iopub.status.idle":"2024-03-03T19:25:54.899749Z","shell.execute_reply":"2024-03-03T19:25:54.898994Z","shell.execute_reply.started":"2024-03-03T19:25:52.418593Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n","from sklearn.model_selection import train_test_split\n","import transformers\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","import copy\n","import datasets\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline\n",")\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{},"source":["# Specify the path to your Excel file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:34:37.588616Z","iopub.status.busy":"2024-03-03T19:34:37.587763Z","iopub.status.idle":"2024-03-03T19:34:37.743609Z","shell.execute_reply":"2024-03-03T19:34:37.742635Z","shell.execute_reply.started":"2024-03-03T19:34:37.588584Z"},"trusted":true},"outputs":[],"source":["\n","# Train set\n","PATH = \"/kaggle/input/465analysisis/Our_trainset.xlsx\"\n","\n","try:\n","    trainset= pd.read_excel(PATH)\n","\n","    print(trainset.head())\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:35:35.098641Z","iopub.status.busy":"2024-03-03T19:35:35.097783Z","iopub.status.idle":"2024-03-03T19:35:35.141053Z","shell.execute_reply":"2024-03-03T19:35:35.140164Z","shell.execute_reply.started":"2024-03-03T19:35:35.098608Z"},"trusted":true},"outputs":[],"source":["\n","# Test Set\n","PATH = \"/kaggle/input/465analysisis/Our_testset.xlsx\"\n","\n","try:\n","    testset = pd.read_excel(PATH)\n","\n","    print(testset.head())\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:35:39.360569Z","iopub.status.busy":"2024-03-03T19:35:39.359886Z","iopub.status.idle":"2024-03-03T19:35:39.451056Z","shell.execute_reply":"2024-03-03T19:35:39.450165Z","shell.execute_reply.started":"2024-03-03T19:35:39.360533Z"},"trusted":true},"outputs":[],"source":["\n","# Validation Set\n","PATH = \"/kaggle/input/465analysisis/Our_validationset.xlsx\"\n","\n","try:\n","    validationset = pd.read_excel(PATH)\n","\n","    print(validationset.head())\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Chunk-based Pre-Processing "]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:36:30.079660Z","iopub.status.busy":"2024-03-03T19:36:30.078745Z","iopub.status.idle":"2024-03-03T19:36:30.084699Z","shell.execute_reply":"2024-03-03T19:36:30.083808Z","shell.execute_reply.started":"2024-03-03T19:36:30.079627Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","# Instantiate RecursiveCharacterTextSplitter with custom parameters\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=2000,\n","    chunk_overlap=200,\n","    length_function=len,\n","    #separators=['\\n\\n','MP:', '\\n', ' ', '']\n",")\n","\n","# Function to split text into chunks using the text splitter\n","def split_text_into_chunks(text):\n","    return text_splitter.split_text(text)"]},{"cell_type":"markdown","metadata":{},"source":["# Create a list to store chunked data"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:39:30.729022Z","iopub.status.busy":"2024-03-03T19:39:30.728651Z","iopub.status.idle":"2024-03-03T19:39:33.790933Z","shell.execute_reply":"2024-03-03T19:39:33.790101Z","shell.execute_reply.started":"2024-03-03T19:39:30.728993Z"},"trusted":true},"outputs":[],"source":["# Train Data\n","train_df = pd.DataFrame({\"Paper\": trainset[\"Main_Paper\"], \"Meta_Paper\": trainset[\"SP\"]})\n","\n","chunked_train_data = {\"Paper\": [], \"Meta_Paper\": []}\n","\n","for i, row in train_df.iterrows():\n","    context = row[\"Meta_Paper\"]\n","    label = row[\"Paper\"]\n","    #print(len(context))\n","    \n","    # Split the context into overlapping chunks\n","    chunks = split_text_into_chunks(context)\n","    for chunk in chunks:\n","        chunked_train_data[\"Paper\"].append(label)\n","        chunked_train_data[\"Meta_Paper\"].append(chunk)\n","\n","# Create a new DataFrame with the chunked data\n","chunked_train_df = pd.DataFrame(chunked_train_data)\n","\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:39:41.684550Z","iopub.status.busy":"2024-03-03T19:39:41.683779Z","iopub.status.idle":"2024-03-03T19:39:41.695042Z","shell.execute_reply":"2024-03-03T19:39:41.694116Z","shell.execute_reply.started":"2024-03-03T19:39:41.684517Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Paper</th>\n","      <th>Meta_Paper</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Background  The available evidence about the p...</td>\n","      <td>MP: ObjectiveSeveral randomized controlled tri...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Background  The available evidence about the p...</td>\n","      <td>hormonal and inflammatory parameters, or can i...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Background  The available evidence about the p...</td>\n","      <td>and synbiotics supplementation in lipid profil...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Background  The available evidence about the p...</td>\n","      <td>and meta-analysis. Mean Difference (MD) was po...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Background  The available evidence about the p...</td>\n","      <td>on homeostatic model assessment-insulin resist...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3835</th>\n","      <td>Available  on the association between the Medi...</td>\n","      <td>incidents, histologically confirmed gastric ca...</td>\n","    </tr>\n","    <tr>\n","      <th>3836</th>\n","      <td>The effective management of the 33 million chi...</td>\n","      <td>MP: Despite the decreasing rate of under nutri...</td>\n","    </tr>\n","    <tr>\n","      <th>3837</th>\n","      <td>The effective management of the 33 million chi...</td>\n","      <td>No adverse reactions were observed. There were...</td>\n","    </tr>\n","    <tr>\n","      <th>3838</th>\n","      <td>The effective management of the 33 million chi...</td>\n","      <td>Three-factor analysis of covariance of the eff...</td>\n","    </tr>\n","    <tr>\n","      <th>3839</th>\n","      <td>The effective management of the 33 million chi...</td>\n","      <td>at least one episode of SAM (MUAC &lt;11cm and/or...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3840 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                                  Paper  \\\n","0     Background  The available evidence about the p...   \n","1     Background  The available evidence about the p...   \n","2     Background  The available evidence about the p...   \n","3     Background  The available evidence about the p...   \n","4     Background  The available evidence about the p...   \n","...                                                 ...   \n","3835  Available  on the association between the Medi...   \n","3836  The effective management of the 33 million chi...   \n","3837  The effective management of the 33 million chi...   \n","3838  The effective management of the 33 million chi...   \n","3839  The effective management of the 33 million chi...   \n","\n","                                             Meta_Paper  \n","0     MP: ObjectiveSeveral randomized controlled tri...  \n","1     hormonal and inflammatory parameters, or can i...  \n","2     and synbiotics supplementation in lipid profil...  \n","3     and meta-analysis. Mean Difference (MD) was po...  \n","4     on homeostatic model assessment-insulin resist...  \n","...                                                 ...  \n","3835  incidents, histologically confirmed gastric ca...  \n","3836  MP: Despite the decreasing rate of under nutri...  \n","3837  No adverse reactions were observed. There were...  \n","3838  Three-factor analysis of covariance of the eff...  \n","3839  at least one episode of SAM (MUAC <11cm and/or...  \n","\n","[3840 rows x 2 columns]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["chunked_train_df"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:40:07.411693Z","iopub.status.busy":"2024-03-03T19:40:07.411360Z","iopub.status.idle":"2024-03-03T19:40:08.836653Z","shell.execute_reply":"2024-03-03T19:40:08.835868Z","shell.execute_reply.started":"2024-03-03T19:40:07.411668Z"},"trusted":true},"outputs":[],"source":["# Validation Data\n","val_df = pd.DataFrame({\"Paper\": validationset[\"Main_Paper\"], \"Meta_Paper\": validationset[\"SP\"]})\n","\n","\n","chunked_val_data = {\"Paper\": [], \"Meta_Paper\": []}\n","\n","for i, row in val_df.iterrows():\n","    context = row[\"Meta_Paper\"]\n","    label = row[\"Paper\"]\n","\n","    # Split the context into overlapping chunks\n","    chunks = split_text_into_chunks(context)\n","    for chunk in chunks:\n","        chunked_val_data[\"Paper\"].append(label)\n","        chunked_val_data[\"Meta_Paper\"].append(chunk)\n","\n","# Create a new DataFrame with the chunked data\n","chunked_val_df = pd.DataFrame(chunked_val_data)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:40:15.747475Z","iopub.status.busy":"2024-03-03T19:40:15.746660Z","iopub.status.idle":"2024-03-03T19:40:16.147674Z","shell.execute_reply":"2024-03-03T19:40:16.146710Z","shell.execute_reply.started":"2024-03-03T19:40:15.747442Z"},"trusted":true},"outputs":[],"source":["# test data\n","test_df = pd.DataFrame({\"Paper\": testset[\"Main_Paper\"], \"Meta_Paper\": testset[\"SP\"]})\n","\n","chunked_test_data = {\"Paper\": [], \"Meta_Paper\": []}\n","\n","for i, row in test_df.iterrows():\n","    context = row[\"Meta_Paper\"]\n","    label = row[\"Paper\"]\n","\n","    # Split the context into overlapping chunks\n","    chunks = split_text_into_chunks(context)\n","    for chunk in chunks:\n","        chunked_test_data[\"Paper\"].append(label)\n","        chunked_test_data[\"Meta_Paper\"].append(chunk)\n","\n","# Create a new DataFrame with the chunked data\n","chunked_test_df = pd.DataFrame(chunked_test_data)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:40:23.093580Z","iopub.status.busy":"2024-03-03T19:40:23.092956Z","iopub.status.idle":"2024-03-03T19:40:23.288462Z","shell.execute_reply":"2024-03-03T19:40:23.287689Z","shell.execute_reply.started":"2024-03-03T19:40:23.093550Z"},"trusted":true},"outputs":[],"source":["trainset = datasets.Dataset.from_dict(chunked_train_df)\n","valset = datasets.Dataset.from_dict(chunked_val_df)\n","testset = datasets.Dataset.from_dict(chunked_test_df)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T03:07:39.947315Z","iopub.status.busy":"2024-03-03T03:07:39.946976Z","iopub.status.idle":"2024-03-03T03:07:39.960078Z","shell.execute_reply":"2024-03-03T03:07:39.959142Z","shell.execute_reply.started":"2024-03-03T03:07:39.947291Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T03:07:41.020874Z","iopub.status.busy":"2024-03-03T03:07:41.020127Z","iopub.status.idle":"2024-03-03T03:07:41.044161Z","shell.execute_reply":"2024-03-03T03:07:41.043173Z","shell.execute_reply.started":"2024-03-03T03:07:41.020842Z"},"trusted":true},"outputs":[],"source":["notebook_login()"]},{"cell_type":"markdown","metadata":{},"source":["# Load Model with QLoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:40:27.610393Z","iopub.status.busy":"2024-03-03T19:40:27.610040Z","iopub.status.idle":"2024-03-03T19:41:59.909218Z","shell.execute_reply":"2024-03-03T19:41:59.908143Z","shell.execute_reply.started":"2024-03-03T19:40:27.610366Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import torch\n","\n","nf4_config = BitsAndBytesConfig(\n","   load_in_4bit=True,\n","   bnb_4bit_quant_type=\"nf4\",\n","   bnb_4bit_use_double_quant=True,\n","   bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    device_map='auto',\n","    quantization_config=nf4_config,\n","    use_cache=False\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{},"source":["# Initialize Instruction based Fine-tuning"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:42:20.175181Z","iopub.status.busy":"2024-03-03T19:42:20.174257Z","iopub.status.idle":"2024-03-03T19:42:20.179376Z","shell.execute_reply":"2024-03-03T19:42:20.178399Z","shell.execute_reply.started":"2024-03-03T19:42:20.175147Z"},"trusted":true},"outputs":[],"source":["DEFAULT_SYSTEM_PROMPT = \"\"\"\n","Given a collection of abstracts from papers used in various medical fields, generate a meta-analysis abstract summarizing the key findings of those abstracts and provide numerical values or statistical information for specific observations that are commonly reported in the provided abstracts. Some provided abstracts may have chunks, so maintain information similarities.\n","\"\"\".strip()"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:42:27.443244Z","iopub.status.busy":"2024-03-03T19:42:27.442889Z","iopub.status.idle":"2024-03-03T19:42:27.449739Z","shell.execute_reply":"2024-03-03T19:42:27.448775Z","shell.execute_reply.started":"2024-03-03T19:42:27.443215Z"},"trusted":true},"outputs":[],"source":["from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n","\n","def formatting_prompts_func(example):\n","    output_texts = []\n","    for i in range(len(example['Meta_Paper'])):\n","        text = f\"### {DEFAULT_SYSTEM_PROMPT},### Abstracts: {example['Meta_Paper'][i]}  \\n ### Meta-Analysis Abstract: {example['Paper'][i]}\"\n","        \n","\n","        output_texts.append(text)\n","    return output_texts\n","\n"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:43:02.109874Z","iopub.status.busy":"2024-03-03T19:43:02.109219Z","iopub.status.idle":"2024-03-03T19:43:02.114676Z","shell.execute_reply":"2024-03-03T19:43:02.113769Z","shell.execute_reply.started":"2024-03-03T19:43:02.109843Z"},"trusted":true},"outputs":[],"source":["from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:47:34.851371Z","iopub.status.busy":"2024-03-03T19:47:34.850384Z","iopub.status.idle":"2024-03-03T19:47:35.402752Z","shell.execute_reply":"2024-03-03T19:47:35.401385Z","shell.execute_reply.started":"2024-03-03T19:47:34.851327Z"},"trusted":true},"outputs":[],"source":["model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:47:52.262651Z","iopub.status.busy":"2024-03-03T19:47:52.262267Z","iopub.status.idle":"2024-03-03T19:47:52.267148Z","shell.execute_reply":"2024-03-03T19:47:52.266094Z","shell.execute_reply.started":"2024-03-03T19:47:52.262621Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:47:59.318610Z","iopub.status.busy":"2024-03-03T19:47:59.317946Z","iopub.status.idle":"2024-03-03T19:47:59.324866Z","shell.execute_reply":"2024-03-03T19:47:59.323866Z","shell.execute_reply.started":"2024-03-03T19:47:59.318577Z"},"trusted":true},"outputs":[],"source":["def softmax_selection(predictions, temperature=1.0, dim=-1):\n","    \"\"\"\n","    Apply softmax to model predictions and sample a token based on the resulting probabilities.\n","\n","    Args:\n","        predictions (torch.Tensor): The tensor containing the raw predictions from the model.\n","        temperature (float): Temperature parameter to adjust the sharpness of the probability distribution.\n","                              A lower temperature makes the distribution sharper.\n","\n","    Returns:\n","        torch.Tensor: Tensor containing the selected token IDs.\n","    \"\"\"\n","   # print(predictions.dim())\n","    if predictions.dim() > 2:\n","        predictions = predictions.view(-1, predictions.size(-1))\n","    # Apply softmax with temperature\n","    probs = F.softmax(predictions / temperature, dim=-1)\n","  \n","\n","    # Sampling a token based on the probabilities\n","    sampled_tokens = torch.multinomial(probs, 1)\n","\n","    return sampled_tokens"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:48:03.318341Z","iopub.status.busy":"2024-03-03T19:48:03.317969Z","iopub.status.idle":"2024-03-03T19:48:03.326250Z","shell.execute_reply":"2024-03-03T19:48:03.325177Z","shell.execute_reply.started":"2024-03-03T19:48:03.318310Z"},"trusted":true},"outputs":[],"source":["def quantities(decoded_predictions, decoded_labels):\n","    \"\"\"\n","    Extract quantities from decoded predictions and labels.\n","\n","    Args:\n","        decoded_predictions (list): List of decoded predictions.\n","        decoded_labels (list): List of decoded labels.\n","\n","    Returns:\n","        tuple: A tuple containing lists of predicted quantities and actual quantities.\n","    \"\"\"\n","    predicted_quantities = []\n","    actual_quantities = []\n","\n","    for prediction, label in zip(decoded_predictions, decoded_labels):\n","        # Placeholder: Extracting the first number encountered in the decoded strings\n","        predicted_quantity = extract_quantity(prediction)\n","        actual_quantity = extract_quantity(label)\n","        predicted_quantities.append(predicted_quantity)\n","        actual_quantities.append(actual_quantity)\n","\n","    return predicted_quantities, actual_quantities\n","\n","def extract_quantity(text):\n","    \"\"\"\n","    Extract the first numeric value from a string.\n","\n","    Args:\n","        text (str): Input string.\n","\n","    Returns:\n","        float: Extracted numeric value or NaN if no numeric value found.\n","    \"\"\"\n","    import re\n","    # Regular expression to match numeric values\n","    numeric_pattern = re.compile(r'[-+]?[0-9]*\\.?[0-9]+')\n","    # Search for numeric values in the text\n","    match = re.search(numeric_pattern, text)\n","    if match:\n","        # Extract and convert the first numeric value found\n","        return float(match.group())\n","    else:\n","        # Return NaN if no numeric value found\n","        return float('nan')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T19:56:38.565118Z","iopub.status.busy":"2024-03-03T19:56:38.564689Z","iopub.status.idle":"2024-03-03T19:56:38.592356Z","shell.execute_reply":"2024-03-03T19:56:38.591405Z","shell.execute_reply.started":"2024-03-03T19:56:38.565086Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import datasets\n","#dataset['tokenized_label'] = llama_tokenizer(dataset['Label'], return_tensors='pt')\n","trainset, testset = train_test_split(\n","    dataset,\n","    test_size=0.2\n",")\n","trainset = datasets.Dataset.from_dict(trainset)\n","testset = datasets.Dataset.from_dict(testset)\n","type(trainset)"]},{"cell_type":"markdown","metadata":{},"source":["# Inverse Cosine Distance (ICD) Loss function implementation "]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T03:17:55.233242Z","iopub.status.busy":"2024-03-03T03:17:55.232861Z","iopub.status.idle":"2024-03-03T03:17:55.244788Z","shell.execute_reply":"2024-03-03T03:17:55.243638Z","shell.execute_reply.started":"2024-03-03T03:17:55.233212Z"},"trusted":true},"outputs":[],"source":["\n","\n","class CustomSFTTrainer(SFTTrainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        \n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # Extract and remove labels from inputs\n","        labels = inputs.get(\"labels\")\n","        \n","        # Forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.logits  # Assuming the model outputs logits\n","        \n","        # Ensure logits and labels are float tensors\n","        logits = logits.float()\n","        labels = labels.float()\n","        \n","        # Reshape logits and labels\n","        logits = logits.view(logits.size(0), logits.size(1), -1)  \n","        labels = labels.view(labels.size(0), labels.size(1), -1)  \n","        \n","        # Normalize embeddings for cosine similarity\n","        logits = F.normalize(logits, p=2, dim=-1)\n","        labels = F.normalize(labels, p=2, dim=-1)\n","\n","        # Calculate cosine similarity\n","        cosine_sim = torch.cosine_similarity(logits, labels, dim=-1)\n","        \n","        # Calculate loss using inverse of cosine similarity with epsilon for numerical stability\n","        epsilon = torch.tensor(1e-8, device=logits.device)\n","        loss = torch.mean(1 / (cosine_sim + epsilon))\n","\n","        return (loss, outputs) if return_outputs else loss\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T03:19:11.261206Z","iopub.status.busy":"2024-03-03T03:19:11.260271Z","iopub.status.idle":"2024-03-03T03:19:30.969322Z","shell.execute_reply":"2024-03-03T03:19:30.968365Z","shell.execute_reply.started":"2024-03-03T03:19:11.261162Z"},"trusted":true},"outputs":[],"source":["# Loading LORA weights\n","base_model.config.use_cache = False\n","\n","from peft import prepare_model_for_kbit_training\n","PATH1 = \"/finetuned_mistral_cosine_sim_p1\"\n","\n","from peft import PeftModel, PeftConfig\n","base_model = PeftModel.from_pretrained(base_model, PATH1)\n","# ---- ending LORA weights -----\n","\n","# Training Params\n","train_params = TrainingArguments(\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=1,\n","    fp16=False,\n","    max_grad_norm=0.3,\n","    num_train_epochs=2,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=0.2,\n","    warmup_ratio=0.05,\n","    save_strategy=\"epoch\",\n","    group_by_length=True,\n","    save_safetensors=True,\n","    lr_scheduler_type=\"constant\",\n","    output_dir=\"./results_modified_test\",\n","    #num_train_epochs=1,\n","    #per_device_train_batch_size=2,\n","    #gradient_accumulation_steps=1,\n","    optim=\"paged_adamw_32bit\",\n","    save_steps=2,\n","    logging_steps=2,\n","    learning_rate=2e-4,\n","    weight_decay=0.001,\n","    #fp16=False,\n","    bf16=False,\n","    #max_grad_norm=0.3,\n","    max_steps=-1,\n","    #warmup_ratio=0.05,\n","    #group_by_length=True,\n","    #lr_scheduler_type=\"constant\",\n","    report_to=\"wandb\",   \n","    # push to hub parameters,\n","   # push_to_hub=True,\n",")\n","\n","# Trainer\n","fine_tuning = CustomSFTTrainer(\n","    model=base_model,\n","    train_dataset=trainset,\n","    eval_dataset = valset,\n","    peft_config=peft_parameters,\n","    # dataset_text_field=train_dataset[\"Statement\"],\n","    tokenizer=llama_tokenizer,\n","    args=train_params,\n","    #max_seq_length = 2048,\n","    formatting_func=formatting_prompts_func,\n","    # data_collator=collator,\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T03:19:40.396434Z","iopub.status.busy":"2024-03-03T03:19:40.395713Z","iopub.status.idle":"2024-03-03T13:58:01.084409Z","shell.execute_reply":"2024-03-03T13:58:01.083400Z","shell.execute_reply.started":"2024-03-03T03:19:40.396403Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbas3235\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240303_031946-o7cawmot</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jbas3235/llama2_465finetune22/runs/o7cawmot' target=\"_blank\">honest-yogurt-1</a></strong> to <a href='https://wandb.ai/jbas3235/llama2_465finetune22' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jbas3235/llama2_465finetune22' target=\"_blank\">https://wandb.ai/jbas3235/llama2_465finetune22</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jbas3235/llama2_465finetune22/runs/o7cawmot' target=\"_blank\">https://wandb.ai/jbas3235/llama2_465finetune22/runs/o7cawmot</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3840/3840 10:37:34, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>768</td>\n","      <td>1.045200</td>\n","      <td>0.970265</td>\n","    </tr>\n","    <tr>\n","      <td>1536</td>\n","      <td>1.042100</td>\n","      <td>0.967326</td>\n","    </tr>\n","    <tr>\n","      <td>2304</td>\n","      <td>1.038000</td>\n","      <td>0.964995</td>\n","    </tr>\n","    <tr>\n","      <td>3072</td>\n","      <td>1.039200</td>\n","      <td>0.964744</td>\n","    </tr>\n","    <tr>\n","      <td>3840</td>\n","      <td>0.895600</td>\n","      <td>0.964325</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=3840, training_loss=1.0367341736331581, metrics={'train_runtime': 38294.4796, 'train_samples_per_second': 0.201, 'train_steps_per_second': 0.1, 'total_flos': 1.5332062395777024e+17, 'train_loss': 1.0367341736331581, 'epoch': 2.0})"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["fine_tuning.train()"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:17:19.698573Z","iopub.status.busy":"2024-03-03T14:17:19.697708Z","iopub.status.idle":"2024-03-03T14:17:19.820199Z","shell.execute_reply":"2024-03-03T14:17:19.819029Z","shell.execute_reply.started":"2024-03-03T14:17:19.698540Z"},"trusted":true},"outputs":[],"source":["fine_tuning.save_model(\"finetuned_mistral_cosine_sim_p1\")"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:18:09.913261Z","iopub.status.busy":"2024-03-03T14:18:09.912871Z","iopub.status.idle":"2024-03-03T14:18:09.932038Z","shell.execute_reply":"2024-03-03T14:18:09.930938Z","shell.execute_reply.started":"2024-03-03T14:18:09.913234Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): LlamaForCausalLM(\n","      (model): LlamaModel(\n","        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n","        (layers): ModuleList(\n","          (0-31): 32 x LlamaDecoderLayer(\n","            (self_attn): LlamaAttention(\n","              (q_proj): Linear4bit(\n","                in_features=4096, out_features=4096, bias=False\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=4, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=4, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","              (v_proj): Linear4bit(\n","                in_features=4096, out_features=4096, bias=False\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=4, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=4, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","              (rotary_emb): LlamaRotaryEmbedding()\n","            )\n","            (mlp): LlamaMLP(\n","              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n","              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n","              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n","              (act_fn): SiLUActivation()\n","            )\n","            (input_layernorm): LlamaRMSNorm()\n","            (post_attention_layernorm): LlamaRMSNorm()\n","          )\n","        )\n","        (norm): LlamaRMSNorm()\n","      )\n","      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n","    )\n","  )\n",")"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["fine_tuning.model"]},{"cell_type":"markdown","metadata":{},"source":["# Meta-analysis Generation "]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:21:25.239814Z","iopub.status.busy":"2024-03-03T14:21:25.238943Z","iopub.status.idle":"2024-03-03T14:21:25.246922Z","shell.execute_reply":"2024-03-03T14:21:25.245948Z","shell.execute_reply.started":"2024-03-03T14:21:25.239778Z"},"trusted":true},"outputs":[],"source":["def summarize(text: str):\n","    inputs = llama_tokenizer(text, return_token_type_ids=False,return_tensors=\"pt\").to(\"cuda\")\n","#     inputs_length = len(inputs[\"input_ids\"][0])\n","    with torch.inference_mode():\n","        outputs = base_model.generate(**inputs, max_new_tokens=1024, do_sample=True, temperature=0.7)\n","\n","#         answer_tokens = outputs[:, inputs.input_ids.shape[1] :]\n","        decoded_output = llama_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","        \n","#         output_text = llama_tokenizer.decode(answer_tokens[0], skip_special_tokens=True).strip()\n","    return decoded_output"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:21:28.952705Z","iopub.status.busy":"2024-03-03T14:21:28.951924Z","iopub.status.idle":"2024-03-03T14:21:28.958539Z","shell.execute_reply":"2024-03-03T14:21:28.957467Z","shell.execute_reply.started":"2024-03-03T14:21:28.952664Z"},"trusted":true},"outputs":[],"source":["from peft import prepare_model_for_kbit_training"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:21:29.725671Z","iopub.status.busy":"2024-03-03T14:21:29.725017Z","iopub.status.idle":"2024-03-03T14:21:29.732261Z","shell.execute_reply":"2024-03-03T14:21:29.731443Z","shell.execute_reply.started":"2024-03-03T14:21:29.725636Z"},"trusted":true},"outputs":[],"source":["from peft import PeftModel, PeftConfig"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:21:44.174627Z","iopub.status.busy":"2024-03-03T14:21:44.173931Z","iopub.status.idle":"2024-03-03T14:21:44.180046Z","shell.execute_reply":"2024-03-03T14:21:44.178859Z","shell.execute_reply.started":"2024-03-03T14:21:44.174595Z"},"trusted":true},"outputs":[],"source":["output_dir=\"/kaggle/working/finetuned_mistral_cosine_sim_p1\""]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:21:46.286614Z","iopub.status.busy":"2024-03-03T14:21:46.285926Z","iopub.status.idle":"2024-03-03T14:21:46.407918Z","shell.execute_reply":"2024-03-03T14:21:46.406850Z","shell.execute_reply.started":"2024-03-03T14:21:46.286583Z"},"trusted":true},"outputs":[],"source":["model_t = PeftModel.from_pretrained(base_model, output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:32:04.559172Z","iopub.status.busy":"2024-03-03T14:32:04.558485Z"},"trusted":true},"outputs":[],"source":["# Initialize an empty list to store rows\n","rows = []\n","\n","# Iterate over the texts in testset['Meta_Paper']\n","for i in range(len(testset['Meta_Paper'])):\n","    text = f\"### {DEFAULT_SYSTEM_PROMPT},### Example Abstracts: {testset['Meta_Paper'][i]}  \\n ### Meta-Analysis Abstract: \"\n","    processed_output = summarize(text)\n","    \n","    # Extract the abstract from processed_output list\n","    abstract = \"\"\n","    for item in processed_output:\n","        if '### Meta-Analysis Abstract:' in item:\n","            abstract = item.split('### Meta-Analysis Abstract:')[1].strip()\n","            break\n","    \n","    # Append the \"Meta_Paper\" value and its processed output as a tuple to the rows list\n","    rows.append((testset['Meta_Paper'][i], abstract))\n","\n","# Create a DataFrame from the rows list\n","df = pd.DataFrame(rows, columns=['Meta_Paper', 'Processed Output'])\n","\n","# Print or use the DataFrame as needed\n","print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.to_csv(\"mistral_finetune_icd.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T20:43:38.676173Z","iopub.status.busy":"2024-03-01T20:43:38.675781Z","iopub.status.idle":"2024-03-01T20:55:14.719639Z","shell.execute_reply":"2024-03-01T20:55:14.718108Z","shell.execute_reply.started":"2024-03-01T20:43:38.676140Z"},"trusted":true},"outputs":[],"source":["for i in range(len(testset['Meta_Paper'])):\n","    text = f\"### {DEFAULT_SYSTEM_PROMPT},### Example Abstracts: {testset['Meta_Paper'][i]}  \\n ### Meta-Analysis Abstract: \"\n","    pred = []\n","    processed_output = summarize(text)\n","    print(processed_output)\n","    print(\"\\n\\n\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4533315,"sourceId":7753155,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
